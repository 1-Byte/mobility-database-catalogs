name: Export catalogs to CSV

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9]
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v2
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest wheel numpy
          sudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable
          sudo apt-get update
          sudo apt-get install gdal-bin python3-gdal
          sudo apt-get install libgdal-dev
          pip install GDAL==$(gdal-config --version) --global-option=build_ext --global-option="-I/usr/include/gdal"
          sudo apt-get install libspatialindex-dev
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      - name: Export the catalog of sources as CSV
        uses: jannekem/run-python-script-action@v1
        with:
          script: |
            import pandas as pd
            import os
            import json

            CSV_PATH = "./catalog_sources.csv"
            CSV_COLUMNS = [
              'mdb_source_id',
              'data_type',
              'location.country_code',
              'location.subdivision_name',
              'location.municipality',
              'provider',
              'name',
              'static_reference',
              'urls.auto_discovery',
              'urls.latest',
              'urls.license',
              'urls.realtime_vehicle_positions',
              'urls.realtime_trip_updates',
              'urls.realtime_alerts',
              'location.bounding_box.minimum_latitude',
              'location.bounding_box.maximum_latitude',
              'location.bounding_box.minimum_longitude',
              'location.bounding_box.maximum_longitude',
              'location.bounding_box.extracted_on'
            ]

            # tools.constants.GTFS
            GTFS = "gtfs"

            # tools.constants.GTFS_RT
            GTFS = "gtfs_rt"

            # tools.constants.MDB_SOURCE_ID
            MDB_SOURCE_ID = "mdb_source_id"

            # tools.constants.GTFS_SCHEDULE_CATALOG_PATH_FROM_ROOT
            GTFS_SCHEDULE_CATALOG_PATH_FROM_ROOT = "catalogs/sources/gtfs/schedule"

            # tools.constants.GTFS_REALTIME_CATALOG_PATH_FROM_ROOT
            GTFS_REALTIME_CATALOG_PATH_FROM_ROOT = "catalogs/sources/gtfs/realtime"

            # tools.operations.get_sources
            gtfs_schedule_catalog_path = os.path.join(".", GTFS_SCHEDULE_CATALOG_PATH_FROM_ROOT)
            gtfs_realtime_catalog_path = os.path.join(".", GTFS_REALTIME_CATALOG_PATH_FROM_ROOT)
            catalog = {}
                for catalog_path in [gtfs_schedule_catalog_path, gtfs_realtime_catalog_path]
                    for path, sub_dirs, files in os.walk(catalog_path):
                        for file in files:
                            with open(os.path.join(path, file)) as fp:
                                entity_json = json.load(fp)
                                entity_id = entity_json[MDB_SOURCE_ID]
                                catalog[entity_id] = entity_json
            catalog = dict(sorted(catalog.items()))

            # tools.helpers.to_csv
            path = CSV_PATH
            columns = CSV_COLUMNS
            catalog = pd.json_normalize(catalog)
            if columns is not None:
                catalog = catalog[columns]
            catalog.to_csv(path, sep=",", index=False)

      - name: Upload the catalog of sources CSV artifact
        uses: actions/upload-artifact@v1
        with:
          name: catalog-sources-csv-v${{ github.run_id }}.${{ github.run_number }}
          path: ./catalog_sources.csv