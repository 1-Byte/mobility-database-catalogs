name: Store latest dataset on approval

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  pre_ci:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9]
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v2
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest wheel numpy
          sudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable
          sudo apt-get update
          sudo apt-get install gdal-bin python3-gdal
          sudo apt-get install libgdal-dev
          pip install GDAL==$(gdal-config --version) --global-option=build_ext --global-option="-I/usr/include/gdal"
          sudo apt-get install libspatialindex-dev
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
  get_urls:
    needs: [ pre_ci ]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Get added and modified files
        id: files
        uses: jitterbit/get-changed-files@v1
      - name: Create a matrix with the auto-discovery and latest urls
        uses: jannekem/run-python-script-action@v1
        with:
          script: |
            import os
            import json
            import numpy as np

            # OS constants
            ROOT = os.getcwd()
            GTFS_SCHEDULE_CATALOG_PATH_FROM_ROOT = "catalogs/sources/gtfs/schedule"

            # File constants
            URLS = "urls"
            AUTO_DISCOVERY = "auto_discovery"
            LATEST = "latest"

            # Github constants
            MAX_JOB_NUMBER = 256

            # Matrix constants
            INCLUDE = "include"
            DATA = "data"
            BASE = "base"

            changed_files = "${{ steps.files.outputs.all }}".split()
            changed_files = [file for file in changed_files if GTFS_SCHEDULE_CATALOG_PATH_FROM_ROOT in file]

            urls = {}
            for file in changed_files:
                base = os.path.splitext(os.path.basename(file))[0]
                with open(os.path.join(ROOT, file)) as fp:
                    file_json = json.load(fp)
                    auto_discovery_url = file_json.get(URLS, {}).get(AUTO_DISCOVERY)
                    latest_url = file_json.get(URLS, {}).get(LATEST)
                    if auto_discovery_url is None:
                        raise ValueError(f"File {file} is missing its auto-discovery url.")
                    if latest_url is None:
                        raise ValueError(f"File {file} is missing its latest url.")
                    urls[base] = {AUTO_DISCOVERY: auto_discovery_url, LATEST: latest_url}

            urls_data = []
            jobs = np.array_split(list(urls.keys()), min(MAX_JOB_NUMBER, len(list(urls.keys()))))
            jobs = [list(job) for job in jobs]
            for job in jobs:
                urls_data_string = ""
                while len(job) > 0:
                    file_base = job.pop()
                    file_information = {
                        BASE: file_base,
                        AUTO_DISCOVERY: urls[file_key][AUTO_DISCOVERY],
                        LATEST: urls[file_key][LATEST]
                    }
                    urls_data_string = urls_data_string + json.dumps(
                        file_information, separators=(",", ":")
                    )
                job_data = {DATA: urls_data_string.replace("}{", "} {")}
                urls_data.append(job_data)
            matrix_data = {INCLUDE: urls_data}
            print(matrix_data)
