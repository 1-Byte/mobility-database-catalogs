name: Store latest datasets cronjob

on:
  pull_request:
    branches: [ main ]
  schedule:
    - cron: "0 0 * * *"

jobs:
  get-urls:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python 3.9
        uses: actions/setup-python@v2
        with:
          python-version: 3.9
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install wheel numpy
      - name: Get added and modified files
        id: files
        uses: jitterbit/get-changed-files@v1
      - name: Create URLs matrix with the auto-discovery and latest URLs
        shell: python
        run: |
          import os
          import json
          import numpy as np

          # OS constants
          ROOT = os.getcwd()
          GTFS_SCHEDULE_CATALOG_PATH_FROM_ROOT = "catalogs/sources/gtfs/schedule"
          MATRIX_FILE = "urls_matrix.json"

          # File constants
          URLS = "urls"
          AUTO_DISCOVERY = "auto_discovery"
          LATEST = "latest"

          # Github constants
          MAX_JOB_NUMBER = 256

          # Matrix constants
          INCLUDE = "include"
          DATA = "data"
          BASE = "base"

          # Report constants
          get_urls_report = "get_urls_report.txt"

          files = os.listdir(os.path.join(ROOT, GTFS_SCHEDULE_CATALOG_PATH_FROM_ROOT))

          urls = {}
          for file in files:
              base = os.path.splitext(os.path.basename(file))[0]
              with open(os.path.join(ROOT, GTFS_SCHEDULE_CATALOG_PATH_FROM_ROOT, file), "r") as fp:
                  file_json = json.load(fp)
                  auto_discovery_url = file_json.get(URLS, {}).get(AUTO_DISCOVERY)
                  latest_url = file_json.get(URLS, {}).get(LATEST)
                  if auto_discovery_url is None or latest_url is None:
                      file_log = (
                          f"{base}: FAILURE! Auto-discovery URL is {auto_discovery_url} and latest URL is {latest_url}. "
                          f"Both URLS must be defined to update the source latest URL.\n"
                      )
                  else:
                      file_log = (
                          f"{base}: SUCCESS! Both auto-discovery and latest URLs are defined.\n"
                      )
                  with open(get_urls_report, "a") as fp:
                      fp.write(file_log)
                  urls[base] = {AUTO_DISCOVERY: auto_discovery_url, LATEST: latest_url}

          urls_data = []
          jobs = np.array_split(list(urls.keys()), min(MAX_JOB_NUMBER, len(list(urls.keys()))))
          jobs = [list(job) for job in jobs]
          for job in jobs:
              urls_data_string = ""
              while len(job) > 0:
                  file_base = job.pop()
                  file_information = {
                      BASE: file_base,
                      AUTO_DISCOVERY: urls[file_base][AUTO_DISCOVERY],
                      LATEST: urls[file_base][LATEST]
                  }
                  urls_data_string = urls_data_string + json.dumps(
                      file_information, separators=(",", ":")
                  )
              job_data = {DATA: urls_data_string.replace("}{", "} {")}
              urls_data.append(job_data)
          matrix_data = {INCLUDE: urls_data}

          with open(os.path.join(ROOT, MATRIX_FILE), "w") as fp:
              file_json = json.dump(matrix_data, fp)
      - name: Set URLs matrix
        id: set-matrix
        run: |
          DATASETS=$(jq . ./urls_matrix.json -c)
          echo $DATASETS
          echo "::set-output name=matrix::$DATASETS"
      - name: Persist URLs matrix artifact
        if: always()
        uses: actions/upload-artifact@v2
        with:
          name: urls_matrix
          path: ./urls_matrix.json
      - name: Persist Get URLS report artifact
        if: always()
        uses: actions/upload-artifact@v2
        with:
          name: get_urls_report
          path: ./get_urls_report.txt
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
